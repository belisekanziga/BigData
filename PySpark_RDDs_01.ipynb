{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a579b3bd-6eb6-4445-8bb9-82fb1dc1e072",
    "deepnote_cell_type": "markdown",
    "id": "TseOmaeVepJP"
   },
   "source": [
    "# PySpark\n",
    "\n",
    "![Logo](https://github.com/pnavaro/big-data/blob/master/notebooks/images/apache_spark_logo.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-bf31e048-ace9-4993-8da2-5f1af4015c57",
    "deepnote_cell_type": "markdown",
    "id": "7MCoeao9epJV"
   },
   "source": [
    "- [Apache Spark](https://spark.apache.org) was first released in 2014.\n",
    "- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n",
    "- Spark is written in [Scala](https://www.scala-lang.org).\n",
    "- All images come from [Databricks](https://databricks.com/product/getting-started-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-bb8dce6d-d8e9-486b-818d-f6dcc250b446",
    "deepnote_cell_type": "markdown",
    "id": "azcYAKk-epJX"
   },
   "source": [
    "- Apache Spark is a fast and general-purpose cluster computing system.\n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.\n",
    "- Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "- It also supports a rich set of higher-level tools including [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) for SQL and structured data processing, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) for machine learning, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-d67dd6f1-d704-4caa-81cf-5a9071b62f17",
    "deepnote_cell_type": "markdown",
    "id": "Ps7SkNSEepJZ"
   },
   "source": [
    "## Resilient distributed datasets\n",
    "\n",
    "- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n",
    "- RDDs behave a bit like Python collections (e.g. lists).\n",
    "- When working with Apache Spark we iteratively apply functions to every item of these collections in parallel to produce *new* RDDs.\n",
    "- The data is distributed across nodes in a cluster of computers.\n",
    "- Functions implemented in Spark can work in parallel across elements of the collection.\n",
    "- The  Spark framework allocates data and processing to different nodes, without any intervention from the programmer.\n",
    "- RDDs automatically rebuilt on machine failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-befe75de-d238-405e-81e7-38cedbf082ce",
    "deepnote_cell_type": "markdown",
    "id": "Eu2f4twKepJc"
   },
   "source": [
    "## Lifecycle of a Spark Program\n",
    "\n",
    "1. Create some input RDDs from external data or parallelize a collection in your driver program.\n",
    "2. Lazily transform them to define new RDDs using transformations like `filter()` or `map()`\n",
    "3. Ask Spark to cache() any intermediate RDDs that will need to be reused.\n",
    "4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-7b21b99e-f856-4c8e-9be8-3c9e9f4e9630",
    "deepnote_cell_type": "markdown",
    "id": "dHTv00_bepJd"
   },
   "source": [
    "## Operations on Distributed Data\n",
    "\n",
    "- Two types of operations: **transformations** and **actions**\n",
    "- Transformations are *lazy* (not computed immediately)\n",
    "- Transformations are executed when an action is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-d114d429-bb93-4c93-affc-216589f7db5e",
    "deepnote_cell_type": "markdown",
    "id": "IjykGZWnepJe"
   },
   "source": [
    "## [Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n",
    "\n",
    "```\n",
    "map() flatMap()\n",
    "filter()\n",
    "mapPartitions() mapPartitionsWithIndex()\n",
    "sample()\n",
    "union() intersection() distinct()\n",
    "groupBy() groupByKey()\n",
    "reduceBy() reduceByKey()\n",
    "sortBy() sortByKey()\n",
    "join()\n",
    "cogroup()\n",
    "cartesian()\n",
    "pipe()\n",
    "coalesce()\n",
    "repartition()\n",
    "partitionBy()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-c19d8245-7c9f-407b-8518-0cdbb557be79",
    "deepnote_cell_type": "markdown",
    "id": "rOn-vwfGepJi"
   },
   "source": [
    "## [Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "\n",
    "```\n",
    "reduce()\n",
    "collect()#To see the result\n",
    "count()\n",
    "first()\n",
    "take()\n",
    "takeSample()\n",
    "saveToCassandra()\n",
    "takeOrdered()\n",
    "saveAsTextFile()\n",
    "saveAsSequenceFile()\n",
    "saveAsObjectFile()\n",
    "countByKey()\n",
    "foreach()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-385816d2-d5dd-4556-a0ec-2bbc0d0f54dc",
    "deepnote_cell_type": "markdown",
    "id": "FuZ1QjYwepJk"
   },
   "source": [
    "## Python API\n",
    "\n",
    "PySpark uses Py4J that enables Python programs to dynamically access Java objects.\n",
    "\n",
    "![PySpark Internals](https://github.com/pnavaro/big-data/blob/master/notebooks/images/YlI8AqEl.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-f66ff6fd-edde-4c4c-bfa1-ba0d1d96ba97",
    "deepnote_cell_type": "markdown",
    "id": "joYETQsSepJl"
   },
   "source": [
    "## The `SparkContext` class\n",
    "\n",
    "- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.SparkContext` context.\n",
    "\n",
    "- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n",
    "\n",
    "- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n",
    "    - normally we would create an RDD from a large file or an HBase table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-a1e30157-0efd-490d-8b63-459dfb96da8a",
    "deepnote_cell_type": "markdown",
    "id": "MOO8pH3iepJm"
   },
   "source": [
    "## First example\n",
    "\n",
    "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter.\n",
    "\n",
    "We have a spark context sc to use with a tiny local spark cluster with 4 nodes (will work just fine on a multicore machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cell_id": "00012-8698152c-385e-4d22-ae56-408429b08600",
    "deepnote_cell_type": "code",
    "execution_millis": 4,
    "execution_start": 1606208122779,
    "id": "RvBvcPCsepJn",
    "output_cleared": false,
    "source_hash": "a22d9657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guest/anaconda3/bin/python'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "cell_id": "00013-7e179656-f878-4ef7-bfc5-4683fd92851d",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1606208260172,
    "id": "qKGv5gJVepJp",
    "output_cleared": false,
    "source_hash": "a04577ae"
   },
   "outputs": [],
   "source": [
    "#os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "cell_id": "00014-f661aeba-0a4e-489c-93b7-034ee40d1ef4",
    "deepnote_cell_type": "code",
    "execution_millis": 7637,
    "execution_start": 1606208269332,
    "id": "OBl1KVwcepJq",
    "output_cleared": false,
    "source_hash": "73659aa7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=FirstExample, master=local[*]) created by __init__ at /tmp/ipykernel_9979/3732607580.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      5\u001b[0m spark\u001b[38;5;241m=\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 7\u001b[0m sc \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkContext(master\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m, appName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirstExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m sc\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pyspark/core/context.py:457\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    454\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    462\u001b[0m             currentAppName,\n\u001b[1;32m    463\u001b[0m             currentMaster,\n\u001b[1;32m    464\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    465\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    466\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    467\u001b[0m         )\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=FirstExample, master=local[*]) created by __init__ at /tmp/ipykernel_9979/3732607580.py:5 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc = pyspark.SparkContext(master=\"local[*]\", appName=\"FirstExample\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cell_id": "00015-bf748379-3532-453e-8058-f4fc6531a830",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1606208288902,
    "id": "5oeERBbrepJr",
    "output_cleared": false,
    "source_hash": "3831e90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=FirstExample>\n"
     ]
    }
   ],
   "source": [
    "print(sc) # it is like a Pool Processor executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-192e38d9-530b-4b44-82f2-0e98ab5d8379",
    "deepnote_cell_type": "markdown",
    "id": "Dh_BwkCHepJs"
   },
   "source": [
    "## Create your first RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(range(8))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "cell_id": "00017-6079893b-b1de-4a98-91ee-cbc10b34cb56",
    "deepnote_cell_type": "code",
    "execution_millis": 549,
    "execution_start": 1606208314201,
    "id": "mKeWSz-6epJs",
    "output_cleared": false,
    "source_hash": "27883792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd = sc.parallelize(data) # create collection\n",
    "rdd\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-3d4cb001-a63e-4efa-a375-656a819b62f7",
    "deepnote_cell_type": "markdown",
    "id": "z1pHV-AyepJs"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Create a file `./Datasets/WhoAreWe.txt` with the text bellow. Read and load it into a RDD with the `textFile` spark function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The African Institute for Mathematical Sciences (AIMS) is a pan-African network of Centres of Excellence for postgraduate training in mathematical sciences, research, and public engagement in Science, Technology, Engineering, and Mathematics. Founded in 2003 in South Africa by acclaimed physicist Prof Neil Turok and later replicated in Senegal, Ghana, Cameroon and Rwanda, AIMS is leading Africa’s socio-economic transformation through:\n",
    "\n",
    "Innovative scientific training (the development of human capital);\n",
    "\n",
    "Technological advances and cutting-edge scientific discoveries; and\n",
    "\n",
    "Public engagement for the continent’s scientific emergence.\n",
    "\n",
    "Africa’s youth are at the heart of the AIMS innovation and transformation ecosystem which consists of a set of academic and non-academic programs expertly tailored to provide AIMS learners with a unique postgraduate training experience on the continent.\n",
    "\n",
    "AIMS offers a Master’s in mathematical sciences, including a co-operative option with a direct link to industry, the African Master’s in Machine Intelligence (AMMI), as well as research programs, with over 100 researchers conducting studies across the network. In addition to the AIMS Industry Initiative and a gender-responsive Teacher Training Program currently implemented in Cameroon and Rwanda, AIMS equally created two critical initiatives: Quantum Leap Africa, a think tank looking into the coming quantum revolution and the Next Einstein Forum to propel Africa on to the global scientific stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aggregating_DataFrames_in_PySpark.ipynb\n",
      " Archive.zip\n",
      " fifa19.csv\n",
      " googleplaystore.csv\n",
      " Handling_Missing_Data_in_PySpark.ipynb\n",
      " __MACOSX\n",
      " nyc_air_bnb.csv\n",
      " people.json\n",
      " pga_not_partitioned.parquet\n",
      " pga_partitioned_by_season.parquet\n",
      " pga_tour_historical.csv\n",
      " PySpark_RDDs_02_updated.ipynb\n",
      "'Python vs PySpark.ipynb'\n",
      " Read_Write_and_Validate_Data.ipynb\n",
      " rec-crime-pfa.csv\n",
      " Rep_vs_Dem_tweets.csv\n",
      "'Search and Filter DataFrames in PySpark.ipynb'\n",
      " SQL_Options_in_Spark.ipynb\n",
      " students.csv\n",
      " supermarket_sales.csv\n",
      " users1.parquet\n",
      " users2.parquet\n",
      " users3.parquet\n",
      " users_today\n",
      " uw-madision-courses\n",
      " Weather.csv.zip\n",
      " WhoAreWe.txt\n",
      " youtubevideos.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!ls Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the file using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `textFile` from the `Datasets/` folder using the function `textFile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The African Institute for Mathematical Sciences (AIMS) is a pan-African network of Centres of Excellence for postgraduate training in mathematical sciences, research, and public engagement in Science, Technology, Engineering, and Mathematics. Founded in 2003 in South Africa by acclaimed physicist Prof Neil Turok and later replicated in Senegal, Ghana, Cameroon and Rwanda, AIMS is leading Africa’s socio-economic transformation through:',\n",
       " '',\n",
       " '',\n",
       " 'Innovative scientific training (the development of human capital);',\n",
       " '',\n",
       " '',\n",
       " 'Technological advances and cutting-edge scientific discoveries; and',\n",
       " '',\n",
       " '',\n",
       " 'Public engagement for the continent’s scientific emergence.',\n",
       " '',\n",
       " '',\n",
       " 'Africa’s youth are at the heart of the AIMS innovation and transformation ecosystem which consists of a set of academic and non-academic programs expertly tailored to provide AIMS learners with a unique postgraduate training experience on the continent.',\n",
       " '',\n",
       " '',\n",
       " 'AIMS offers a Master’s in mathematical sciences, including a co-operative option with a direct link to industry, the African Master’s in Machine Intelligence (AMMI), as well as research programs, with over 100 researchers conducting studies across the network. In addition to the AIMS Industry Initiative and a gender-responsive Teacher Training Program currently implemented in Cameroon and Rwanda, AIMS equally created two critical initiatives: Quantum Leap Africa, a think tank looking into the coming quantum revolution and the Next Einstein Forum to propel Africa on to the global scientific stage.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"Datasets/\"\n",
    "rdd = sc.textFile(path+\"WhoAreWe.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.core.rdd.RDD"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.textFile(path+\"users1.parquet\")\n",
    "rdd.collect()\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-f8161a29-5442-40bc-b3ad-314ac079b925",
    "deepnote_cell_type": "markdown",
    "id": "eiKXwbzVepJt"
   },
   "source": [
    "### Collect\n",
    "\n",
    "Action / To Driver: Return all items in the RDD to the driver in a single list\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/DUO6ygB.png?raw=1)\n",
    "\n",
    "Source: https://i.imgur.com/DUO6ygB.png\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Collect the text you read before from the `WhoAreWe.txt`file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-88914dd9-d7ed-435c-988d-b333620a63f9",
    "deepnote_cell_type": "markdown",
    "id": "K7Bf6BKHepJu"
   },
   "source": [
    "### Map\n",
    "\n",
    "Transformation / Narrow: Return a new RDD by applying a function to each element of this RDD\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/PxNJf0U.png?raw=1)\n",
    "\n",
    "Source: http://i.imgur.com/PxNJf0U.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "def calc_sqrt(x):\n",
    "    return x**2\n",
    "\n",
    "#f=lambda x:x**2\n",
    "print(calc_sqrt(12))\n",
    "#print(lambda x:x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "cell_id": "00022-e19738f1-5568-4e01-a918-31f95720f79a",
    "deepnote_cell_type": "code",
    "execution_millis": 2412,
    "execution_start": 1605183503558,
    "id": "DMaj43GKepJu",
    "output_cleared": false,
    "source_hash": "8210a3cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(range(8)),numSlices=3)#write list of 0-7 and devide into three cluster\n",
    "rdd.map(lambda x: x ** 2).collect() # take element of rdd and Square each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=sc.parallelize(list(range(8)),numSlices=3)\n",
    "rdd1.map(lambda x: x**2).collect()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=list(range(8))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda l:l**2,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sc.parallelize(list(range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.map(lambda l:l**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-096cde31-3814-4827-a025-e528f1021b55",
    "deepnote_cell_type": "markdown",
    "id": "Ges3znO1epJv"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Replace the lambda function by a function that contains a pause (sleep(1)) and check if the `map` operation is parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_square(x):\n",
    "    time.sleep(1)   # Pause for 1 second\n",
    "    return x * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [0, 1, 4, 9, 16, 25, 36, 49]\n",
      "Time taken: 4.068443059921265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(range(8)), numSlices=3)\n",
    "\n",
    "start = time.time()\n",
    "result = rdd.map(slow_square).collect()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Result:\", result)\n",
    "print(\"Time taken:\", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-0a7d639a-570b-49d3-a71a-7330ec203bc8",
    "deepnote_cell_type": "markdown",
    "id": "eAOfDfvBepJv"
   },
   "source": [
    "### Filter\n",
    "\n",
    "Transformation / Narrow: Return a new RDD containing only the elements that satisfy a predicate\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/GFyji4U.png?raw=1)\n",
    "Source: http://i.imgur.com/GFyji4U.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_id": "00025-9893a281-e7fd-49e4-880e-c18c223437e2",
    "deepnote_cell_type": "code",
    "id": "g6t-uJi0epJv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-c2044f27-66c2-49e4-a2c3-1551769fc2fe",
    "deepnote_cell_type": "markdown",
    "id": "-GG5SvGGepJw"
   },
   "source": [
    "### FlatMap\n",
    "\n",
    "Transformation / Narrow: Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/TsSUex8.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "00027-eab04ee2-2189-474e-b0ee-7cdf04d51334",
    "deepnote_cell_type": "code",
    "execution_millis": 1423,
    "execution_start": 1606209096798,
    "id": "PjifIDDmepJz",
    "output_cleared": false,
    "source_hash": "ed69af29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100, 42, 2, 200, 42, 3, 300, 42]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.flatMap(lambda x: (x, x*100, 42)).collect()#apply on each and make them on same row take x itself anf its function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 100, 42), (2, 200, 42), (3, 300, 42)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: (x, x*100, 42)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-d3078029-dd5d-4852-a84f-4fb134597c5d",
    "deepnote_cell_type": "markdown",
    "id": "4sU99s5HepLs"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Use FlatMap to clean the text from `WhoAreWe.txt`file. Lower, remove dots and split into words.\n",
    "\n",
    "### GroupBy\n",
    "\n",
    "Transformation / Wide: Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/gdj0Ey8.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The African Institute for Mathematical Sciences (AIMS) is a pan-African network of Centres of Excellence for postgraduate training in mathematical sciences, research, and public engagement in Science, Technology, Engineering, and Mathematics. Founded in 2003 in South Africa by acclaimed physicist Prof Neil Turok and later replicated in Senegal, Ghana, Cameroon and Rwanda, AIMS is leading Africa’s socio-economic transformation through:',\n",
       " '',\n",
       " '',\n",
       " 'Innovative scientific training (the development of human capital);',\n",
       " '']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"Datasets/\"\n",
    "rdd = sc.textFile(path+\"WhoAreWe.txt\")\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'African',\n",
       " 'Institute',\n",
       " 'for',\n",
       " 'Mathematical',\n",
       " 'Sciences',\n",
       " '(AIMS)',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pan-African',\n",
       " 'network',\n",
       " 'of',\n",
       " 'Centres',\n",
       " 'of',\n",
       " 'Excellence',\n",
       " 'for',\n",
       " 'postgraduate',\n",
       " 'training',\n",
       " 'in',\n",
       " 'mathematical',\n",
       " 'sciences,',\n",
       " 'research,',\n",
       " 'and',\n",
       " 'public',\n",
       " 'engagement',\n",
       " 'in',\n",
       " 'Science,',\n",
       " 'Technology,',\n",
       " 'Engineering,',\n",
       " 'and',\n",
       " 'Mathematics.',\n",
       " 'Founded',\n",
       " 'in',\n",
       " '2003',\n",
       " 'in',\n",
       " 'South',\n",
       " 'Africa',\n",
       " 'by',\n",
       " 'acclaimed',\n",
       " 'physicist',\n",
       " 'Prof',\n",
       " 'Neil',\n",
       " 'Turok',\n",
       " 'and',\n",
       " 'later',\n",
       " 'replicated',\n",
       " 'in',\n",
       " 'Senegal,',\n",
       " 'Ghana,',\n",
       " 'Cameroon',\n",
       " 'and',\n",
       " 'Rwanda,',\n",
       " 'AIMS',\n",
       " 'is',\n",
       " 'leading',\n",
       " 'Africa’s',\n",
       " 'socio-economic',\n",
       " 'transformation',\n",
       " 'through:',\n",
       " '',\n",
       " '',\n",
       " 'Innovative',\n",
       " 'scientific',\n",
       " 'training',\n",
       " '(the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'human',\n",
       " 'capital);',\n",
       " '',\n",
       " '',\n",
       " 'Technological',\n",
       " 'advances',\n",
       " 'and',\n",
       " 'cutting-edge',\n",
       " 'scientific',\n",
       " 'discoveries;',\n",
       " 'and',\n",
       " '',\n",
       " '',\n",
       " 'Public',\n",
       " 'engagement',\n",
       " 'for',\n",
       " 'the',\n",
       " 'continent’s',\n",
       " 'scientific',\n",
       " 'emergence.',\n",
       " '',\n",
       " '',\n",
       " 'Africa’s',\n",
       " 'youth',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'the',\n",
       " 'AIMS',\n",
       " 'innovation',\n",
       " 'and',\n",
       " 'transformation',\n",
       " 'ecosystem',\n",
       " 'which',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'academic',\n",
       " 'and',\n",
       " 'non-academic',\n",
       " 'programs',\n",
       " 'expertly',\n",
       " 'tailored',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'AIMS',\n",
       " 'learners',\n",
       " 'with',\n",
       " 'a',\n",
       " 'unique',\n",
       " 'postgraduate',\n",
       " 'training',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'the',\n",
       " 'continent.',\n",
       " '',\n",
       " '',\n",
       " 'AIMS',\n",
       " 'offers',\n",
       " 'a',\n",
       " 'Master’s',\n",
       " 'in',\n",
       " 'mathematical',\n",
       " 'sciences,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'co-operative',\n",
       " 'option',\n",
       " 'with',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'link',\n",
       " 'to',\n",
       " 'industry,',\n",
       " 'the',\n",
       " 'African',\n",
       " 'Master’s',\n",
       " 'in',\n",
       " 'Machine',\n",
       " 'Intelligence',\n",
       " '(AMMI),',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'research',\n",
       " 'programs,',\n",
       " 'with',\n",
       " 'over',\n",
       " '100',\n",
       " 'researchers',\n",
       " 'conducting',\n",
       " 'studies',\n",
       " 'across',\n",
       " 'the',\n",
       " 'network.',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'AIMS',\n",
       " 'Industry',\n",
       " 'Initiative',\n",
       " 'and',\n",
       " 'a',\n",
       " 'gender-responsive',\n",
       " 'Teacher',\n",
       " 'Training',\n",
       " 'Program',\n",
       " 'currently',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'Cameroon',\n",
       " 'and',\n",
       " 'Rwanda,',\n",
       " 'AIMS',\n",
       " 'equally',\n",
       " 'created',\n",
       " 'two',\n",
       " 'critical',\n",
       " 'initiatives:',\n",
       " 'Quantum',\n",
       " 'Leap',\n",
       " 'Africa,',\n",
       " 'a',\n",
       " 'think',\n",
       " 'tank',\n",
       " 'looking',\n",
       " 'into',\n",
       " 'the',\n",
       " 'coming',\n",
       " 'quantum',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Next',\n",
       " 'Einstein',\n",
       " 'Forum',\n",
       " 'to',\n",
       " 'propel',\n",
       " 'Africa',\n",
       " 'on',\n",
       " 'to',\n",
       " 'the',\n",
       " 'global',\n",
       " 'scientific',\n",
       " 'stage.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"Datasets/\"\n",
    "rdd = sc.textFile(path+\"WhoAreWe.txt\")\n",
    "rdd.collect()\n",
    "rdd.flatMap(lambda B: B.split(\" \")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rdd.flatMap(lambda B: B.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39m text_rdd\u001b[38;5;241m.\u001b[39mmap(flatMap)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "a= text_rdd.map(flatMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.glom func(cunting at end of each line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'pyspark',\n",
       " 'in',\n",
       " 'big',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'with',\n",
       " 'python']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am learning pyspark in big data analysis with python\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I'],\n",
       " ['', ''],\n",
       " ['a'],\n",
       " ['m'],\n",
       " ['', ''],\n",
       " ['l'],\n",
       " ['e'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['n'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " ['', ''],\n",
       " ['p'],\n",
       " ['y'],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['k'],\n",
       " ['', ''],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['', ''],\n",
       " ['b'],\n",
       " ['i'],\n",
       " ['g'],\n",
       " ['', ''],\n",
       " ['d'],\n",
       " ['a'],\n",
       " ['t'],\n",
       " ['a'],\n",
       " ['', ''],\n",
       " ['a'],\n",
       " ['n'],\n",
       " ['a'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " ['s'],\n",
       " ['i'],\n",
       " ['s'],\n",
       " ['', ''],\n",
       " ['w'],\n",
       " ['i'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['', ''],\n",
       " ['p'],\n",
       " ['y'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['o'],\n",
       " ['n']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda C:C.split(\" \"),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cell_id": "00029-93936a32-6fc4-4c60-b38d-7935aa2df61f",
    "deepnote_cell_type": "code",
    "id": "vQU8RnjFepLt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('J', ['John', 'James']), ('F', ['Fred']), ('A', ['Anna'])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "rdd = rdd.groupBy(lambda w: w[0])\n",
    "[(k, list(v)) for (k, v) in rdd.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=rdd.collect()\n",
    "k, v=a[0]\n",
    "list(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-dd9efd67-bc72-4eb5-9e07-0a94bd233e13",
    "deepnote_cell_type": "markdown",
    "id": "EpVZpTifepLw"
   },
   "source": [
    "### GroupByKey\n",
    "\n",
    "Transformation / Wide: Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/TlWRGr2.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "cell_id": "00031-bc1d6a4c-8103-42a9-a39a-ee8d80680447",
    "deepnote_cell_type": "code",
    "id": "NvrOK-EfepLx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [5, 4]), ('A', [3, 2, 1])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "rdd = rdd.groupByKey()\n",
    "[(j[0], list(j[1])) for j in rdd.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-6cff9fda-3f13-4b2b-9b2e-580ad83ecf5f",
    "deepnote_cell_type": "markdown",
    "id": "i0faKW-2epLy"
   },
   "source": [
    "### Join\n",
    "\n",
    "Transformation / Wide: Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/YXL42Nl.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cell_id": "00033-10e3b4f2-3f6f-4a99-adac-d8bc8c92c2e9",
    "deepnote_cell_type": "code",
    "id": "MOTp4gfyepLy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00034-7746caa4-cfd5-4206-b12d-f0fe01d65803",
    "deepnote_cell_type": "markdown",
    "id": "DdUCiAKaepLy"
   },
   "source": [
    "### Distinct\n",
    "\n",
    "Transformation / Wide: Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/Vqgy2a4.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "cell_id": "00035-fd077e6b-1166-4fca-b5e8-c9a310dc59e7",
    "deepnote_cell_type": "code",
    "id": "0NNxYCztepLz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,3,4])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-fdbf2aa3-158f-44a2-82a3-fab846eb5ab7",
    "deepnote_cell_type": "markdown",
    "id": "e8M_1HLdepLz"
   },
   "source": [
    "### KeyBy\n",
    "\n",
    "Transformation / Narrow: Create a Pair RDD, forming one pair for each item in the original RDD. The pair’s key is calculated from the value via a user-supplied function.\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/nqYhDW5.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-e13e8ce6-ad59-4cb9-a672-336afba96873",
    "deepnote_cell_type": "code",
    "id": "s9tftIZBepL1"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "rdd.keyBy(lambda w: w[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-ee1a58a6-954d-4994-a48a-d5a98ac09051",
    "deepnote_cell_type": "markdown",
    "id": "1ifNoE7_epL1"
   },
   "source": [
    "## Actions\n",
    "\n",
    "### Map-Reduce operation\n",
    "\n",
    "Action / To Driver: Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and return a result to the driver\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/R72uzwX.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "cell_id": "00039-cc090acd-104b-4683-844f-153af8f2956d",
    "deepnote_cell_type": "code",
    "id": "nzUdmWNnepL1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize(list(range(8)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x ** 2).reduce(add) # reduce is an action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-a9e8e4e8-143f-403e-b11c-67b97832198b",
    "deepnote_cell_type": "markdown",
    "id": "xcj6X47cepMG"
   },
   "source": [
    "### Max, Min, Sum, Mean, Variance, Stdev\n",
    "\n",
    "Action / To Driver: Compute the respective function (maximum value, minimum value, sum, mean, variance, or standard deviation) from a numeric RDD\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/HUCtib1.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-e22cf358-c375-48c8-ba0b-5489d848df67",
    "deepnote_cell_type": "markdown",
    "id": "UkrD7-hEepMH"
   },
   "source": [
    "### CountByKey\n",
    "\n",
    "Action / To Driver: Return a map of keys and counts of their occurrences in the RDD\n",
    "\n",
    "![](https://github.com/pnavaro/big-data/blob/master/notebooks/images/jvQTGv6.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "cell_id": "00042-613102f2-fea1-436a-a695-8169201af599",
    "deepnote_cell_type": "code",
    "id": "A8c89PSyepMH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'J': 2, 'F': 1, 'A': 1})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('J', 'James'), ('F','Fred'),\n",
    "                    ('A','Anna'), ('J','John')])\n",
    "\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "cell_id": "00043-a8624fda-21b4-4d84-b41a-0b2f94951a4a",
    "deepnote_cell_type": "code",
    "id": "HedOqxQpepML"
   },
   "outputs": [],
   "source": [
    "# Stop the local spark cluster\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00044-36e25a83-4df8-4b26-b143-ac3cf849110a",
    "deepnote_cell_type": "markdown",
    "id": "TFQwil49epML"
   },
   "source": [
    "### Exercise: Word-count in Apache Spark\n",
    "\n",
    "- Write the sample text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "53ba4a86-7d9e-483d-acb5-582d7555be2b",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.9",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "source_map": [
   13,
   19,
   26,
   33,
   45,
   54,
   62,
   85,
   105,
   113,
   124,
   134,
   143,
   148,
   153,
   160,
   162,
   166,
   170,
   176,
   183,
   197,
   207,
   210,
   216,
   225,
   228,
   236,
   239,
   251,
   255,
   263,
   267,
   275,
   279,
   287,
   290,
   298,
   301,
   311,
   315,
   323,
   331,
   338,
   341,
   347,
   351,
   392,
   398,
   402,
   407,
   413,
   428,
   439,
   443,
   459,
   463,
   467,
   473,
   477,
   493,
   499,
   503,
   509,
   513,
   525
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
